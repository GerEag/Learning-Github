{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-free RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of RL is to optimize the reward subject to system dynamics (state transition function) and the policy. Model-free RL learns a policy using only previously acquired data/experience from the environment. This includes previous states and previous reward.\n",
    "\n",
    "There are two classes of model-free RL: policy search and approximate dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Also called \"replay buffer,\" stores previous experiences as $(s_t,a_t,r_t,s_{t+1})$, (state, action, reward, next state). This is used in off-policy methods to continue to learn from old data even as new data comes in. (For some reason it is necessary to have uncorrelated data?) During the learning phase (separate from gaining experience), past experiences are randomly sampled from this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic\n",
    "\n",
    "Actor-critic methods use separate networks to learn both the Q-function (action-value function) and the policy explicitely. This allows the implementation of continuous observation and action spaces. The policy is updated using policy gradient method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Search\n",
    "\n",
    "Policy search seeks to optimize the expected reward (as all RL does):\n",
    "\n",
    "$$\\text{maximize} \\ \\mathbb{E}\\left [ R(\\pi) \\right ]$$\n",
    "\n",
    "If $J=\\mathbb{E}\\left [ R(\\pi) \\right ]$, then we need to find the gradient in order to optimize it ($\\nabla J$)\n",
    "\n",
    "## Gradient of probabilistic policy\n",
    "\n",
    "## Temporal Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "## TD3\n",
    "\n",
    "Proposed to mitigate the effect of overestimation bias in actor-critic methods [2]. Overestimation bias is a property of Q-learning in which a noisy value estimate. Using imprecise estimates of the value (especially using NN/function approximation) at each update accumulates overestimation. Using target networks for the critic can reduce error over multiple updates, so it delays updating the policy until the value error can be reduced to \"acceptable levels\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Benjamin Recht, \"A Tour of Reinforcement Learning: The View from Continuous Control\", Annual Review of Control, Robotics, and Autonomous Systems (2019).\n",
    "\n",
    "[2] Scott Fujimoto et al., \"Addressing Function Approximation Error in Actor-Critic Methods\", Arxiv (2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL_SB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e64f684287bb1046aa9c312a8d9f57463ed463fe5a019b551aad9cb1c0e7d28a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
