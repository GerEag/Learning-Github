{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-free RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of RL is to optimize the reward subject to system dynamics (state transition function) and the policy. Model-free RL learns a policy using only previously acquired data/experience from the environment. This includes previous states and previous reward.\n",
    "\n",
    "There are two classes of model-free RL: policy search and approximate dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Search\n",
    "\n",
    "Policy search seeks to optimize the expected reward (as all RL does):\n",
    "\n",
    "$$\\text{maximize} \\ \\mathbb{E}\\left [ R(\\pi) \\right ]$$\n",
    "\n",
    "If $J=\\mathbb{E}\\left [ R(\\pi) \\right ]$, then we need to find the gradient in order to optimize it ($\\nabla J$)\n",
    "\n",
    "## Gradient of probabilistic policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Benjamin Recht, \"A Tour of Reinforcement Learning: The View from Continuous Control\", Annual Review of Control, Robotics, and Autonomous Systems (2019)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
