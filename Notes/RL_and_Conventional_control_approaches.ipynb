{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches for combining conventional control methods and RL\n",
    "\n",
    "## Linear control (PD) + agent action\n",
    "\n",
    "One method to combine RL and conventional control is to apply a fixed-gain feedback controller with an added term learned by the RL agent. The gains for the fixed-gain controller can be designed with some knowledge about the system. This requires either a simple system, or a simplified system.\n",
    "\n",
    "## Model-Reference Control and RL\n",
    "\n",
    "Place an RL component into a conventional model reference controller, which contains the response of a nominal reference model to a baseline controller. Can provide some local stability guarantees if the agent can maintain low error between true output and model output. The design of the baseline controller still requires some understanding of the system in order to develop the nominal model used for the reference model. There may be other combined control methods that allow for the utilization of domain knowledge without needing to really understand the system at all.\n",
    "\n",
    "### References\n",
    "\n",
    "Zhang, Qingrui, \"Model-Reference Reinforcement Learning for Collision-Free Tracking Control of Autonomous Surface Vehicles\" *IEEE Transactions on Intelligent Transportation Systems* (2021)\n",
    "\n",
    "## State Feedback\n",
    "You can use something like state feedback or PID control. It really is model-free. No need to understand anything about your system. But you do know something about controls. You are setting up the agent to already \"know\" how to stabilize/regulate the system, it just needs to learn the proper gains to optimize the regulator. This can be done very simply by making the actor network shallow so that its weights are just PID gains (Lawrence, 2020).\n",
    "\n",
    "### References\n",
    "Yeh, Yi-Liang and Yang, Po-Kai, \"Design and Comparison of Reinforcement-Learning-Based Time-Varying PID Controllers with Gain-Scheduled Actions\", *Machines* (2021)\n",
    "\n",
    "Nathan P. Lawrence, \"Optimal PID and Antiwindup Control Design as a Reinforcement Learning Problem\", IFAC-PapersOnLine (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Learned Controllers\n",
    "\n",
    "## Stability\n",
    "Stability is important, but is difficult to quantifity for highly nonlinear systems, especially when the control structor (neural networks) is also nonlinear. The policy is also constantly changing during the learning phase. This also means that global stability cannot generally by guaranteed.\n",
    "\n",
    "One way to approach guaranteeing stability for policies is ensuring that the learned policies stay within some predetermined set of policies. You can also place penalties in the reward function to encourage stability about local equilibria.\n",
    "\n",
    "### References\n",
    "Jin, Ming and Lavaei, Javad, \"Stability-Certified Reinforcement Learning: A Control-Theoretic Perspective\" *IEEE Access* (2020)\n",
    "\n",
    "## Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL_SB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e64f684287bb1046aa9c312a8d9f57463ed463fe5a019b551aad9cb1c0e7d28a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
