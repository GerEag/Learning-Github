{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with Reinforcement Learning\n",
    "\n",
    "## Stability Guarantees\n",
    "There are certain systems/applications for RL where stability is not a concern, such as for games or \"decision problems\" (what is this actually?). For application to robotics, autonomous navigation, etc., it is necessary to have some sort of understanding of the stability of the system subject to the learned policy.\n",
    "\n",
    "### Lyapunov-based approach\n",
    "One approach is to use control Lyapunov functions to design a set of model-based controllers that the agent is tasked with selecting [1]. This method uses discrete actions to switch between stabilizing and does not allow any means of generating continuous optimal policies.\n",
    "\n",
    "There is another method based on Lyapunov functions. A policy can be trained such that starting with a locally stable policy, policy exploration can occur to expand the region of attraction and also never leave the region of attraction [3]. Requires the system to be Lipschitz continuous. This paper takes advantage of the value function in RL/approximate dynamic programming being a good Lyapunov function candidate. This method requires that we have an initial stabilizing policy. This method is also model based. This method guarantees safety, but sacrifices on exploration. I think because this method is risk averse during training, you may end up with a relatively small region of attraction after training. (If I'm doing sim-to-real it's ok to try unstable initial states.)\n",
    "\n",
    "### Q-parameterization\n",
    "Q-parameterization allows a set of stabilizing controllers to be defined around a baseline stabilizing controller. A stable linear controller can be designed for a stable system, then an agent can be added to the baseline controller. Q-parameterization allows for the generation of the action space to allow for the optimization of the controller while maintaining/guaranteeing stability [2]. Does Q-parameterization require the system and stabilizing controller to be linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability [4]\n",
    "Deep RL uses large neural networks with many interconnections between nodes, leading to an architecture that is difficult for a human to interpret and understand intuitively, even though the NN can be represented mathematically. This essentially makes the NN a \"black box.\"\n",
    "\n",
    "### Post-hoc methods\n",
    "The original model is maintained and an additional model/explanation is used to help interpret the original model/policy.\n",
    "\n",
    "### Intrinsic methods\n",
    "The original model is replaced by a more interpretable model.\n",
    "\n",
    " - Decision trees - requires training a shallow model from a deep NN model, often by using imitation learning, (it may be that more complex systems require larger trees, reducing their interpretability) (may also be reserved for discrete state action spaces). There have been some RL algorithms that train decision tree policies, but they are difficult to train [5].\n",
    " - Saliency maps - image that highlights the contribution of individual pixels in the agent's decision (is this method reserved for image based learning?)\n",
    " - Region Sensitive Rainbows - identifies important regions of the input image (definitely for image-based learning)\n",
    " - Counterfactual states - (also image-based)\n",
    " - t-Distributed Stochastic Neighbor Embedding - (image-based)\n",
    " - Summarization methods - requires a human to manually interpret the behavior of the policy. It is not random, but requires humans to follow a format in their explanations (this seems highly time intensive and requires humans to look at a lot of different time responses)\n",
    "   - Can summarize the behavior at critical states\n",
    "   - Manual reconstruction of the agent model\n",
    "   - Automatic aggregation of \"interesting\" behavior\n",
    " - Custom Learning Models - Uses human interpretable language to explain agent\n",
    "   - Algebraic Languages\n",
    "   Custom Domain Programming Language\n",
    "   - Formal Specification Language\n",
    "   - Interpretable Hierarchical RL\n",
    "   - Reconstruction\n",
    "\n",
    "#### Decision Trees\n",
    "Decision trees seem to be the closed thing to a conventional control policy for mechanical systems. It observes states (like position and velocity) and decides an action (potentially piecewise continuous). The main problem seems to be that they are difficult to train, using either supervised learning methods or imitation learning. Also, complex systems (multi-dimensional, continuous) require a number of leaves in the tree that make it practically impossible for a human to interpret globally (you may however be able to interpret small portions of the tree.)\n",
    "Ultimately, there doesn't seem to be any interpretation method strongly based on control theory.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Perkins, Theodore J., and Andrew G. Barto. \"Lyapunov design for safe reinforcement learning.\" Journal of Machine Learning Research 3.Dec (2002): 803-832.\n",
    "\n",
    "[2] S. R. Friedrich and M. Buss, \"A robust stability approach to robot reinforcement learning based on a parameterization of stabilizing controllers,\" 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3365-3372, doi: 10.1109/ICRA.2017.7989382.\n",
    "\n",
    "[3] Berkenkamp, F., Turchetta, M., Schoellig, A., & Krause, A. (2017). Safe model-based reinforcement learning with stability guarantees. Advances in neural information processing systems, 30.\n",
    "\n",
    "[4] A. Alharin, T. -N. Doan and M. Sartipi, \"Reinforcement Learning Interpretation Methods: A Survey,\" in IEEE Access, vol. 8, pp. 171058-171077, 2020, doi: 10.1109/ACCESS.2020.3023394.\n",
    "\n",
    "[5] Bastani, Osbert, Yewen Pu, and Armando Solar-Lezama. \"Verifiable reinforcement learning via policy extraction.\" Advances in neural information processing systems 31 (2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('RL_SB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e64f684287bb1046aa9c312a8d9f57463ed463fe5a019b551aad9cb1c0e7d28a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
